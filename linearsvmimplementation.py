# -*- coding: utf-8 -*-
"""LinearSVMImplementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EPuMl8hOyYrljQuNQgnxblgkm0oGioyb
"""

# Importing the libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score

from google.colab import files
uploaded = files.upload()

from __future__ import print_function
# Import the dataset
dataset = pd.read_csv('hw2_question3.csv', header=None)
full_dataset = dataset.iloc[:, :].values

total_samples = len(dataset)

print('Total number of samples is {}'.format(total_samples))
print('Input shape is {}'.format(dataset.shape))

# https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf
columns_to_hot_encode = [1, 6, 7, 13, 14, 15, 25, 28]

for col in columns_to_hot_encode:
    dataset[str(col)+'_neg'] = np.where(dataset[col] == -1, 1, 0)
    dataset[str(col)+'_zero'] = np.where(dataset[col] == 0, 1, 0)
    dataset[str(col)+'_pos'] = np.where(dataset[col] == 1, 1, 0)

dataset.head(5)

dataset = dataset.drop(columns=[1, 6, 7, 13, 14, 15, 25, 28])
dataset.head(5)

# move the target at the end
df1 = dataset.pop(30)
dataset[30] = df1
dataset.head(5)

# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html
# shuffle the samples
dataset = dataset.sample(frac=1, random_state=9728).reset_index(drop=True)
dataset.head(5)

# divide the sammples

total_train_samples = (int)((2/3)*total_samples)
total_test_samples = total_samples - total_train_samples

print('Total number of training samples is {}'.format(total_train_samples))
print('Total number of test samples is {}'.format(total_test_samples))

training_set = dataset.head(total_train_samples)
test_set = dataset.tail(total_test_samples)

# SVM starts here
training_set_data = training_set.iloc[:, :].values
X = training_set_data[:, :-1]
y = training_set_data[:, 46]

test_set_data = test_set.iloc[:, :].values
X_test = test_set_data[:, :-1]
y_test = test_set_data[:, 46]

solution_outside = []

import cvxopt
import cvxopt.solvers

# http://members.cbio.mines-paristech.fr/~thocking/mines-course/2011-04-01-svm/svm-qp.pdf
# https://xavierbourretsicotte.github.io/SVM_implementation.html
# https://courses.csail.mit.edu/6.867/wiki/images/e/ef/Qp-quadprog.pdf
# http://members.cbio.mines-paristech.fr/~thocking/mines-course/2011-04-01-svm/svm-qp.pdf
# https://stackoverflow.com/questions/15527622/how-to-implement-a-soft-margin-svm-model-using-matlabs-quadprog
# https://stats.stackexchange.com/questions/181305/large-margin-classifier-with-cvxopt
# https://static1.squarespace.com/static/58851af9ebbd1a30e98fb283/t/58902fbae4fcb5398aeb7505/1485844411772/SVM+Explained.pdf
# (MAIN) https://xavierbourretsicotte.github.io/SVM_implementation.html

class SVM(object):

    def __init__(self, C=1.0):
        self.C = float(C)
        
    def fit(self, X, y):
        global solution_outside
        X = np.array(X, dtype=float)
        y = np.array(y, dtype=float)
        print('X is ', X.shape)
        n_samples, n_features = X.shape
        print('y is ',y.shape)
        y_reshaped = y.reshape(-1,1)
        print('y reshaped is ', y_reshaped.shape)
        X_dash = X * y_reshaped 
        H = np.dot(X_dash, X_dash.T)
        q = np.ones(n_samples) * -1.0
        G = np.eye(n_samples) * -1.0
        G = np.vstack((G,np.eye(n_samples)))
        h = np.zeros(n_samples)
        h = np.hstack((h, np.ones(n_samples) * self.C))
        A = y_reshaped.T
        print('A shape is ', A.shape)
        b = np.zeros(1)
        ##############################################
        # solve QP problem
        P = cvxopt.matrix(H)
        q = cvxopt.matrix(q)
        G = cvxopt.matrix(G)
        h = cvxopt.matrix(h)
        A = cvxopt.matrix(A)
        b = cvxopt.matrix(b)

        solution = cvxopt.solvers.qp(P, q, G, h, A, b)
        solution_outside = solution
        #solution = solution_outside
        # Lagrange multipliers
        alphas = np.array(solution['x'])
        print('alphas shape is ', alphas.shape)
        # Support vectors have non zero lagrange multipliers
        sv = (alphas > 1e-4).flatten()
        ind = np.where(alphas > 1e-4)[0]
        self.ind = ind
        self.alphas = alphas[ind]
        self.sv = X[ind]
        self.sv_y = y[ind]
        print("%d support vectors out of %d points" % (len(self.alphas), n_samples))
        
        #==================Computing and printing parameters===============================#
        self.w = ((y_reshaped * alphas).T @ X).reshape(-1,1)
        print('W shape is ', self.w.shape)
        print('W is ', self.w)
        
        self.b = 0
        for n in range(len(self.alphas)):
            self.b += self.sv_y[n]
            self.b -= np.dot(self.sv[n],self.w)
        self.b /= len(self.alphas)

        print('value of b is ', self.b)
        print('indices are ', ind)        
        
    def predict(self, X):
        preds = []
        for sample in X:
            prediction = np.sign(np.dot(sample, self.w) + self.b)
            preds.append(prediction)
        return preds

import time
svm_classifier = SVM(C=2.0)

start = time.time()
svm_classifier.fit(X, y)
end = time.time()

print('Time for training : ', end - start)

y_pred = svm_classifier.predict(X_test)

print(accuracy_score(y_test, y_pred))